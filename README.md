# ArXivTweetBot2

## Overview

![screen shot](image.png)

ArXivTweetBot2 is an automated system that searches for research papers on arXiv based on specified keywords, downloads them, generates easy-to-understand summaries using OpenAI's GPT models, and posts these summaries to Twitter. It also creates a web interface to browse all the summarized papers, organized by date.

The system is designed to run daily via a cron job, searching for new papers matching your configured keywords, and making cutting-edge research accessible to a broader audience through simplified explanations.

## System Architecture

ArXivTweetBot2 consists of several components that work together to automate the process of finding, summarizing, and sharing research papers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  run_daily.sh   â”‚â”€â”€â”€â”€â–¶â”‚run_multiple_    â”‚â”€â”€â”€â”€â–¶â”‚arxiv_downloader.â”‚
â”‚  (Main script)  â”‚     â”‚searches.py      â”‚     â”‚py               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚(Search manager) â”‚     â”‚(Paper processor)â”‚
        â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â”‚                       â”‚                       â–¼
        â”‚                       â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚               â”‚  OpenAI API     â”‚
        â”‚                       â”‚               â”‚  (Summarization)â”‚
        â”‚                       â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â”‚                       â–¼                       â–¼
        â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚twitter_log_     â”‚     â”‚  Twitter API    â”‚
                        â”‚analyzer.py      â”‚     â”‚  (Posting)      â”‚
                        â”‚(Analytics)      â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                                â”‚                       â”‚
                                â–¼                       â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Summary        â”‚     â”‚  web_generator. â”‚
                        â”‚  Reports        â”‚     â”‚  py             â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                â”‚  Web Interface  â”‚
                                                â”‚  (HTML/CSS/JS)  â”‚
                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. `run_daily.sh` is triggered by a cron job and sets up the environment
2. `run_multiple_searches.py` reads the configuration and manages multiple search sets
3. `arxiv_downloader.py` searches arXiv, downloads papers, generates summaries, and posts to Twitter
4. `web_generator.py` creates HTML pages to browse the papers
5. `twitter_log_analyzer.py` analyzes posting logs and generates reports

## Core Components

### arxiv_downloader.py

This is the main workhorse of the system that:

- Searches arXiv API for papers matching specified keywords
- Downloads PDF files of matching papers
- Extracts text from PDFs
- Generates summaries using OpenAI's GPT models
- Posts summaries to Twitter
- Logs all activities and results

**Key Features:**
- Filtering by date and already processed papers
- Customizable summary templates
- Rate limiting for API calls
- Error handling and retry logic
- Detailed logging

**Usage Example:**
```bash
python arxiv_downloader.py "quantum computing" "machine learning" --max-results 20 --since-timestamp "2023-01-01T00:00:00Z"
```

### run_multiple_searches.py

This script manages the execution of multiple search sets defined in the configuration file:

- Reads search sets from config.yaml
- Executes each search set with appropriate parameters
- Manages timestamps to avoid reprocessing papers
- Tracks processed paper IDs across search sets
- Triggers web interface generation

**Key Features:**
- Sequential processing of search sets with configurable delays
- Timestamp-based filtering to only process new papers
- Shared tracking of processed papers across search sets
- Detailed logging of each search set's execution

**Usage Example:**
```bash
python run_multiple_searches.py --all-pages --verbose
```

### web_generator.py

Creates a browsable web interface for all processed papers:

- Generates HTML pages organized by date
- Creates index pages with filtering capabilities
- Includes paper summaries, links to original papers, and Twitter posts
- Supports customizable templates and styling

**Key Features:**
- Responsive design for mobile and desktop viewing
- Search and filter functionality
- Customizable templates
- Support for generating current day only or all historical pages

**Usage Example:**
```bash
python web_generator.py --log-dir ./logs --output-dir /var/www/html/arxiv --template-dir ./templates
```

### twitter_log_analyzer.py

Analyzes Twitter posting logs to generate statistics and reports:

- Processes JSON log files of Twitter posts
- Generates daily and overall posting statistics
- Creates markdown reports and CSV data exports
- Tracks successful and failed posts

**Key Features:**
- Daily posting counts and trends
- Success/failure analysis
- Exportable data in multiple formats
- Customizable report templates

**Usage Example:**
```bash
python twitter_log_analyzer.py --output summary.md --csv data.csv
```

### run_daily.sh

The main shell script that orchestrates the daily execution:

- Sets up the Anaconda environment
- Manages command-line options for execution modes
- Executes the Python scripts in the correct sequence
- Logs all execution steps and results
- Handles errors and reports issues

**Key Features:**
- Support for different execution modes (current day only, all pages, verbose)
- Comprehensive logging
- Environment setup and validation
- Error handling and reporting

**Usage Example:**
```bash
./run_daily.sh --all-pages --verbose
```

## Installation

### Prerequisites
- Python 3.6 or higher
- Anaconda (recommended for environment management)
- Twitter Developer Account (for posting to Twitter)
- OpenAI API key

### Step by Step Installation

1. Clone the repository:
```bash
git clone https://github.com/daishir0/ArXivTweetBot2.git
cd ArXivTweetBot2
```

2. Install the required dependencies:
```bash
pip install -r requirements.txt
```

3. Create a configuration file:
```bash
cp config.sample.yaml config.yaml
```

4. Edit the configuration file with your settings:
```bash
nano config.yaml
```

5. Configure the following in your `config.yaml`:
   - OpenAI API key
   - Twitter API credentials
   - Search keywords and settings
   - Output directories for the web interface

## Configuration File Details

The `config.yaml` file is the central configuration for the system. Here's a detailed explanation of its structure:

```yaml
# API Keys and Authentication
api:
  openai:
    api_key: "your-openai-api-key"
  twitter:
    consumer_key: "your-twitter-consumer-key"
    consumer_secret: "your-twitter-consumer-secret"
    access_token: "your-twitter-access-token"
    access_token_secret: "your-twitter-access-token-secret"

# Execution Settings
execution:
  wait_between_sets: 30  # Seconds to wait between search sets
  current_only: true     # Generate only current day's pages by default

# Search Sets
search_sets:
  - name: "Machine Learning"
    keywords: ["machine learning", "deep learning", "neural network"]
    output_dir: "/var/www/html/llm_rag"
    max_results: 50
    tweet_enabled: true
    
  - name: "Quantum Computing"
    keywords: ["quantum computing", "quantum algorithm"]
    output_dir: "/var/www/html/quantum"
    max_results: 30
    tweet_enabled: true

# Summary Settings
summary:
  model: "gpt-4"          # OpenAI model to use
  temperature: 0.7        # Creativity level (0.0-1.0)
  max_tokens: 500         # Maximum tokens in summary
  language: "en"          # Summary language (en, ja, etc.)
  
# Web Interface Settings
web:
  title: "ArXiv Research Summaries"
  description: "Daily summaries of the latest research papers"
  theme: "light"          # light or dark
  items_per_page: 20
```

## Usage

### Running a Single Search
To search for papers with specific keywords:

```bash
python arxiv_downloader.py "machine learning" "neural networks" --max-results 50
```

### Running Multiple Search Sets
To run multiple predefined search sets from your config file:

```bash
python run_multiple_searches.py
```

### Setting Up Daily Execution
To set up automatic daily execution:

1. Make the script executable:
```bash
chmod +x run_daily.sh
```

2. Add to crontab to run daily (e.g., at 6 AM):
```bash
crontab -e
# Add the following line:
0 6 * * * /path/to/ArXivTweetBot2/run_daily.sh
```

### Generating Web Interface Only
To regenerate the web interface without searching for new papers:

```bash
python web_generator.py --log-dir ./logs --output-dir /var/www/html/arxiv
```

## Advanced Usage Examples

### Specialized Research Field Configuration

For AI Safety research:

```yaml
search_sets:
  - name: "AI Safety"
    keywords: ["AI safety", "AI alignment", "AI ethics", "AI governance"]
    output_dir: "/var/www/html/ai_safety"
    max_results: 100
    tweet_enabled: true
    custom_prompt: "Summarize this AI safety paper with focus on practical implications and potential risks."
```

### Custom Summary Templates

You can customize the summary templates in the configuration:

```yaml
summary_templates:
  default: |
    Title: {title}
    Authors: {authors}
    
    TL;DR: {tldr}
    
    Key points:
    {bullet_points}
    
    #arXiv #Research #AI
  
  technical: |
    ğŸ“‘ {title}
    ğŸ‘©â€ğŸ”¬ {authors}
    
    ğŸ’¡ Summary: {tldr}
    
    ğŸ”‘ Technical details:
    {technical_details}
    
    ğŸ”— {url}
    #arXiv #TechResearch
```

## Troubleshooting

### Common Issues

#### Python Version Errors

If you see syntax errors like:
```
SyntaxError: invalid syntax
```
with f-strings, ensure you're using Python 3.6+. In cron jobs, explicitly specify the Python path:

```bash
# In run_daily.sh
PYTHON_CMD="python3"  # Instead of just "python"
```

#### API Rate Limiting

If you encounter rate limiting:
```
Error: Rate limit exceeded for OpenAI API
```

Solution: Adjust the execution settings to add delays between requests:
```yaml
execution:
  api_request_delay: 5  # Seconds between API calls
```

#### PDF Processing Errors

If PDF text extraction fails:
```
Error processing PDF: Unable to extract text
```

Solution: Install additional dependencies:
```bash
apt-get install poppler-utils
pip install pdfminer.six
```

## Notes
- The system creates several directories for organization:
  - `dl/`: Downloaded PDF files
  - `text/`: Extracted text from PDFs
  - `summary/`: Generated summaries
  - `processed/`: Records of processed papers
  - `logs/`: Execution logs

- Twitter posting can be disabled with the `--skip-twitter` flag if you just want to collect and summarize papers.

- The web interface is generated in the specified output directory and can be served by any web server.

- The system uses a character named "C(ãƒ»Ï‰ãƒ» )ã¤" as a mascot for the Twitter posts and web interface.

## License
This project is licensed under the MIT License - see the LICENSE file for details.

---

# ArXivTweetBot2

## æ¦‚è¦
ArXivTweetBot2ã¯ã€æŒ‡å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦arXivã‹ã‚‰ç ”ç©¶è«–æ–‡ã‚’æ¤œç´¢ã—ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€OpenAIã®GPTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ç†è§£ã—ã‚„ã™ã„è¦ç´„ã‚’ç”Ÿæˆã—ã€ãã®è¦ç´„ã‚’Twitterã«æŠ•ç¨¿ã™ã‚‹è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ã¾ãŸã€è¦ç´„ã•ã‚ŒãŸã™ã¹ã¦ã®è«–æ–‡ã‚’æ—¥ä»˜ã”ã¨ã«æ•´ç†ã—ã¦é–²è¦§ã§ãã‚‹Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚‚ä½œæˆã—ã¾ã™ã€‚

ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯cronã‚¸ãƒ§ãƒ–ã‚’é€šã˜ã¦æ¯æ—¥å®Ÿè¡Œã•ã‚Œã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ãŠã‚Šã€è¨­å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹æ–°ã—ã„è«–æ–‡ã‚’æ¤œç´¢ã—ã€ç°¡ç•¥åŒ–ã•ã‚ŒãŸèª¬æ˜ã‚’é€šã˜ã¦æœ€å…ˆç«¯ã®ç ”ç©¶ã‚’å¹…åºƒã„å±¤ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ã—ã¾ã™ã€‚

## ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

ArXivTweetBot2ã¯ã€ç ”ç©¶è«–æ–‡ã®æ¤œç´¢ã€è¦ç´„ã€å…±æœ‰ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’è‡ªå‹•åŒ–ã™ã‚‹ãŸã‚ã«é€£æºã—ã¦å‹•ä½œã™ã‚‹ã„ãã¤ã‹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  run_daily.sh   â”‚â”€â”€â”€â”€â–¶â”‚run_multiple_    â”‚â”€â”€â”€â”€â–¶â”‚arxiv_downloader.â”‚
â”‚  (ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ)  â”‚     â”‚searches.py      â”‚     â”‚py               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚(æ¤œç´¢ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼)   â”‚     â”‚(è«–æ–‡ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼)  â”‚
        â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â”‚                       â”‚                       â–¼
        â”‚                       â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚               â”‚  OpenAI API     â”‚
        â”‚                       â”‚               â”‚  (è¦ç´„ç”Ÿæˆ)      â”‚
        â”‚                       â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â–¼                       â–¼
        â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚twitter_log_     â”‚     â”‚  Twitter API    â”‚
                        â”‚analyzer.py      â”‚     â”‚  (æŠ•ç¨¿)         â”‚
                        â”‚(åˆ†æ)           â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                                â”‚                       â”‚
                                â–¼                       â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  ã‚µãƒãƒªãƒ¼        â”‚     â”‚  web_generator. â”‚
                        â”‚  ãƒ¬ãƒãƒ¼ãƒˆ        â”‚     â”‚  py             â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                â”‚  Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ â”‚
                                                â”‚  (HTML/CSS/JS)  â”‚
                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

1. `run_daily.sh`ã¯cronã‚¸ãƒ§ãƒ–ã«ã‚ˆã£ã¦ãƒˆãƒªã‚¬ãƒ¼ã•ã‚Œã€ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™
2. `run_multiple_searches.py`ã¯è¨­å®šã‚’èª­ã¿è¾¼ã¿ã€è¤‡æ•°ã®æ¤œç´¢ã‚»ãƒƒãƒˆã‚’ç®¡ç†ã—ã¾ã™
3. `arxiv_downloader.py`ã¯arXivã‚’æ¤œç´¢ã—ã€è«–æ–‡ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€è¦ç´„ã‚’ç”Ÿæˆã—ã€Twitterã«æŠ•ç¨¿ã—ã¾ã™
4. `web_generator.py`ã¯è«–æ–‡ã‚’é–²è¦§ã™ã‚‹ãŸã‚ã®HTMLãƒšãƒ¼ã‚¸ã‚’ä½œæˆã—ã¾ã™
5. `twitter_log_analyzer.py`ã¯æŠ•ç¨¿ãƒ­ã‚°ã‚’åˆ†æã—ã€ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™

## ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

### arxiv_downloader.py

ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã®ä¸»è¦ãªä½œæ¥­ã‚’è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ãŒã‚ã‚Šã¾ã™ï¼š

- æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹è«–æ–‡ã‚’arXiv APIã§æ¤œç´¢
- ä¸€è‡´ã™ã‚‹è«–æ–‡ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
- OpenAIã®GPTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è¦ç´„ã‚’ç”Ÿæˆ
- è¦ç´„ã‚’Twitterã«æŠ•ç¨¿
- ã™ã¹ã¦ã®æ´»å‹•ã¨çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²

**ä¸»ãªæ©Ÿèƒ½ï¼š**
- æ—¥ä»˜ã¨æ—¢ã«å‡¦ç†æ¸ˆã¿ã®è«–æ–‡ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªè¦ç´„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
- APIã‚³ãƒ¼ãƒ«ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™
- ã‚¨ãƒ©ãƒ¼å‡¦ç†ã¨ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯
- è©³ç´°ãªãƒ­ã‚°è¨˜éŒ²

**ä½¿ç”¨ä¾‹ï¼š**
```bash
python arxiv_downloader.py "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°" "æ©Ÿæ¢°å­¦ç¿’" --max-results 20 --since-timestamp "2023-01-01T00:00:00Z"
```

### run_multiple_searches.py

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã•ã‚ŒãŸè¤‡æ•°ã®æ¤œç´¢ã‚»ãƒƒãƒˆã®å®Ÿè¡Œã‚’ç®¡ç†ã—ã¾ã™ï¼š

- config.yamlã‹ã‚‰æ¤œç´¢ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
- é©åˆ‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å„æ¤œç´¢ã‚»ãƒƒãƒˆã‚’å®Ÿè¡Œ
- è«–æ–‡ã®å†å‡¦ç†ã‚’é¿ã‘ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç®¡ç†
- æ¤œç´¢ã‚»ãƒƒãƒˆé–“ã§å‡¦ç†æ¸ˆã¿è«–æ–‡IDã‚’è¿½è·¡
- Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ç”Ÿæˆã‚’ãƒˆãƒªã‚¬ãƒ¼

**ä¸»ãªæ©Ÿèƒ½ï¼š**
- è¨­å®šå¯èƒ½ãªé…å»¶ã‚’æŒã¤æ¤œç´¢ã‚»ãƒƒãƒˆã®é †æ¬¡å‡¦ç†
- æ–°ã—ã„è«–æ–‡ã®ã¿ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- æ¤œç´¢ã‚»ãƒƒãƒˆé–“ã§ã®å‡¦ç†æ¸ˆã¿è«–æ–‡ã®å…±æœ‰è¿½è·¡
- å„æ¤œç´¢ã‚»ãƒƒãƒˆã®å®Ÿè¡Œã®è©³ç´°ãªãƒ­ã‚°è¨˜éŒ²

**ä½¿ç”¨ä¾‹ï¼š**
```bash
python run_multiple_searches.py --all-pages --verbose
```

### web_generator.py

å‡¦ç†ã•ã‚ŒãŸã™ã¹ã¦ã®è«–æ–‡ã®ãƒ–ãƒ©ã‚¦ã‚ºå¯èƒ½ãªWebã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¾ã™ï¼š

- æ—¥ä»˜ã”ã¨ã«æ•´ç†ã•ã‚ŒãŸHTMLãƒšãƒ¼ã‚¸ã‚’ç”Ÿæˆ
- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã‚’æŒã¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’ä½œæˆ
- è«–æ–‡ã®è¦ç´„ã€å…ƒã®è«–æ–‡ã¸ã®ãƒªãƒ³ã‚¯ã€Twitterã®æŠ•ç¨¿ã‚’å«ã‚€
- ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆ

**ä¸»ãªæ©Ÿèƒ½ï¼š**
- ãƒ¢ãƒã‚¤ãƒ«ã¨ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—è¡¨ç¤ºã®ãŸã‚ã®ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³
- æ¤œç´¢ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½
- ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
- ç¾åœ¨ã®æ—¥ä»˜ã®ã¿ã¾ãŸã¯ã™ã¹ã¦ã®å±¥æ­´ãƒšãƒ¼ã‚¸ã®ç”Ÿæˆã®ã‚µãƒãƒ¼ãƒˆ

**ä½¿ç”¨ä¾‹ï¼š**
```bash
python web_generator.py --log-dir ./logs --output-dir /var/www/html/arxiv --template-dir ./templates
```

### twitter_log_analyzer.py

TwitteræŠ•ç¨¿ãƒ­ã‚°ã‚’åˆ†æã—ã¦çµ±è¨ˆã¨ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ï¼š

- TwitteræŠ•ç¨¿ã®JSONãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
- æ—¥æ¬¡ãŠã‚ˆã³å…¨ä½“ã®æŠ•ç¨¿çµ±è¨ˆã‚’ç”Ÿæˆ
- ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã¨CSVãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’ä½œæˆ
- æˆåŠŸã—ãŸæŠ•ç¨¿ã¨å¤±æ•—ã—ãŸæŠ•ç¨¿ã‚’è¿½è·¡

**ä¸»ãªæ©Ÿèƒ½ï¼š**
- æ—¥æ¬¡æŠ•ç¨¿æ•°ã¨ãƒˆãƒ¬ãƒ³ãƒ‰
- æˆåŠŸ/å¤±æ•—åˆ†æ
- è¤‡æ•°ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿
- ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

**ä½¿ç”¨ä¾‹ï¼š**
```bash
python twitter_log_analyzer.py --output summary.md --csv data.csv
```

### run_daily.sh

æ—¥æ¬¡å®Ÿè¡Œã‚’èª¿æ•´ã™ã‚‹ãƒ¡ã‚¤ãƒ³ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼š

- Anacondaç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
- å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ç®¡ç†
- æ­£ã—ã„é †åºã§Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
- ã™ã¹ã¦ã®å®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—ã¨çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²
- ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†ã—ã€å•é¡Œã‚’å ±å‘Š

**ä¸»ãªæ©Ÿèƒ½ï¼š**
- ç•°ãªã‚‹å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®ã‚µãƒãƒ¼ãƒˆï¼ˆç¾åœ¨ã®æ—¥ã®ã¿ã€ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã€è©³ç´°ãƒ¢ãƒ¼ãƒ‰ï¼‰
- åŒ…æ‹¬çš„ãªãƒ­ã‚°è¨˜éŒ²
- ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨æ¤œè¨¼
- ã‚¨ãƒ©ãƒ¼å‡¦ç†ã¨å ±å‘Š

**ä½¿ç”¨ä¾‹ï¼š**
```bash
./run_daily.sh --all-pages --verbose
```

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•

### å‰ææ¡ä»¶
- Python 3.6ä»¥ä¸Š
- Anacondaï¼ˆç’°å¢ƒç®¡ç†ã«æ¨å¥¨ï¼‰
- Twitterãƒ‡ãƒ™ãƒ­ãƒƒãƒ‘ãƒ¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆï¼ˆTwitterã¸ã®æŠ•ç¨¿ç”¨ï¼‰
- OpenAI APIã‚­ãƒ¼

### Step by stepã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•

1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¾ã™ï¼š
```bash
git clone https://github.com/daishir0/ArXivTweetBot2.git
cd ArXivTweetBot2
```

2. å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼š
```bash
pip install -r requirements.txt
```

3. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ï¼š
```bash
cp config.sample.yaml config.yaml
```

4. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã—ã¦è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š
```bash
nano config.yaml
```

5. `config.yaml`ã§ä»¥ä¸‹ã‚’è¨­å®šã—ã¾ã™ï¼š
   - OpenAI APIã‚­ãƒ¼
   - Twitter APIèªè¨¼æƒ…å ±
   - æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨è¨­å®š
   - Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ç”¨ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

## è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°

`config.yaml`ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚·ã‚¹ãƒ†ãƒ ã®ä¸­å¿ƒçš„ãªè¨­å®šã§ã™ã€‚ä»¥ä¸‹ã¯ãã®æ§‹é€ ã®è©³ç´°ãªèª¬æ˜ã§ã™ï¼š

```yaml
# APIã‚­ãƒ¼ã¨èªè¨¼
api:
  openai:
    api_key: "ã‚ãªãŸã®OpenAI-APIã‚­ãƒ¼"
  twitter:
    consumer_key: "ã‚ãªãŸã®Twitterã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ã‚­ãƒ¼"
    consumer_secret: "ã‚ãªãŸã®Twitterã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ"
    access_token: "ã‚ãªãŸã®Twitterã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³"
    access_token_secret: "ã‚ãªãŸã®Twitterã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ"

# å®Ÿè¡Œè¨­å®š
execution:
  wait_between_sets: 30  # æ¤œç´¢ã‚»ãƒƒãƒˆé–“ã®å¾…æ©Ÿç§’æ•°
  current_only: true     # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ç¾åœ¨ã®æ—¥ã®ãƒšãƒ¼ã‚¸ã®ã¿ã‚’ç”Ÿæˆ

# æ¤œç´¢ã‚»ãƒƒãƒˆ
search_sets:
  - name: "æ©Ÿæ¢°å­¦ç¿’"
    keywords: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"]
    output_dir: "/var/www/html/llm_rag"
    max_results: 50
    tweet_enabled: true
    
  - name: "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"
    keywords: ["é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°", "é‡å­ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ "]
    output_dir: "/var/www/html/quantum"
    max_results: 30
    tweet_enabled: true

# è¦ç´„è¨­å®š
summary:
  model: "gpt-4"          # ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«
  temperature: 0.7        # å‰µé€ æ€§ãƒ¬ãƒ™ãƒ«ï¼ˆ0.0-1.0ï¼‰
  max_tokens: 500         # è¦ç´„ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
  language: "ja"          # è¦ç´„è¨€èªï¼ˆenã€jaã€ãªã©ï¼‰
  
# Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¨­å®š
web:
  title: "arXivç ”ç©¶è¦ç´„"
  description: "æœ€æ–°ã®ç ”ç©¶è«–æ–‡ã®æ—¥æ¬¡è¦ç´„"
  theme: "light"          # lightã¾ãŸã¯dark
  items_per_page: 20
```

## ä½¿ã„æ–¹

### å˜ä¸€æ¤œç´¢ã®å®Ÿè¡Œ
ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è«–æ–‡ã‚’æ¤œç´¢ã™ã‚‹ã«ã¯ï¼š

```bash
python arxiv_downloader.py "æ©Ÿæ¢°å­¦ç¿’" "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯" --max-results 50
```

### è¤‡æ•°ã®æ¤œç´¢ã‚»ãƒƒãƒˆã®å®Ÿè¡Œ
è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰äº‹å‰å®šç¾©ã•ã‚ŒãŸè¤‡æ•°ã®æ¤œç´¢ã‚»ãƒƒãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ï¼š

```bash
python run_multiple_searches.py
```

### æ—¥æ¬¡å®Ÿè¡Œã®è¨­å®š
è‡ªå‹•æ—¥æ¬¡å®Ÿè¡Œã‚’è¨­å®šã™ã‚‹ã«ã¯ï¼š

1. ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œå¯èƒ½ã«ã—ã¾ã™ï¼š
```bash
chmod +x run_daily.sh
```

2. crontabã«è¿½åŠ ã—ã¦æ¯æ—¥å®Ÿè¡Œã—ã¾ã™ï¼ˆä¾‹ï¼šåˆå‰6æ™‚ï¼‰ï¼š
```bash
crontab -e
# ä»¥ä¸‹ã®è¡Œã‚’è¿½åŠ ï¼š
0 6 * * * /path/to/ArXivTweetBot2/run_daily.sh
```

### Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ã¿ã®ç”Ÿæˆ
æ–°ã—ã„è«–æ–‡ã‚’æ¤œç´¢ã›ãšã«Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å†ç”Ÿæˆã™ã‚‹ã«ã¯ï¼š

```bash
python web_generator.py --log-dir ./logs --output-dir /var/www/html/arxiv
```

## é«˜åº¦ãªä½¿ç”¨ä¾‹

### å°‚é–€ç ”ç©¶åˆ†é‡ã®è¨­å®š

AIå®‰å…¨æ€§ç ”ç©¶ã®å ´åˆï¼š

```yaml
search_sets:
  - name: "AIå®‰å…¨æ€§"
    keywords: ["AIå®‰å…¨æ€§", "AIã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ", "AIå€«ç†", "AIã‚¬ãƒãƒŠãƒ³ã‚¹"]
    output_dir: "/var/www/html/ai_safety"
    max_results: 100
    tweet_enabled: true
    custom_prompt: "ã“ã® AI å®‰å…¨æ€§è«–æ–‡ã‚’è¦ç´„ã—ã€å®Ÿç”¨çš„ãªæ„å‘³ã¨æ½œåœ¨çš„ãªãƒªã‚¹ã‚¯ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚"
```

### ã‚«ã‚¹ã‚¿ãƒ è¦ç´„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

è¨­å®šã§è¦ç´„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¾ã™ï¼š

```yaml
summary_templates:
  default: |
    ã‚¿ã‚¤ãƒˆãƒ«: {title}
    è‘—è€…: {authors}
    
    è¦ç´„: {tldr}
    
    ãƒã‚¤ãƒ³ãƒˆ:
    {bullet_points}
    
    #arXiv #ç ”ç©¶ #AI
  
  technical: |
    ğŸ“‘ {title}
    ğŸ‘©â€ğŸ”¬ {authors}
    
    ğŸ’¡ è¦ç´„: {tldr}
    
    ğŸ”‘ æŠ€è¡“çš„è©³ç´°:
    {technical_details}
    
    ğŸ”— {url}
    #arXiv #æŠ€è¡“ç ”ç©¶
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ä¸€èˆ¬çš„ãªå•é¡Œ

#### Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼

æ¬¡ã®ã‚ˆã†ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã‚‹å ´åˆï¼š
```
SyntaxError: invalid syntax
```
f-stringã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€Python 3.6ä»¥ä¸Šã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚cronã‚¸ãƒ§ãƒ–ã§ã¯ã€Pythonãƒ‘ã‚¹ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¾ã™ï¼š

```bash
# run_daily.shã§
PYTHON_CMD="python3"  # "python"ã ã‘ã§ã¯ãªã
```

#### APIãƒ¬ãƒ¼ãƒˆåˆ¶é™

ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é­é‡ã—ãŸå ´åˆï¼š
```
Error: Rate limit exceeded for OpenAI API
```

è§£æ±ºç­–ï¼šãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«é…å»¶ã‚’è¿½åŠ ã™ã‚‹ã‚ˆã†ã«å®Ÿè¡Œè¨­å®šã‚’èª¿æ•´ã—ã¾ã™ï¼š
```yaml
execution:
  api_request_delay: 5  # APIã‚³ãƒ¼ãƒ«é–“ã®ç§’æ•°
```

#### PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼

PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãŒå¤±æ•—ã—ãŸå ´åˆï¼š
```
Error processing PDF: Unable to extract text
```

è§£æ±ºç­–ï¼šè¿½åŠ ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼š
```bash
apt-get install poppler-utils
pip install pdfminer.six
```

## æ³¨æ„ç‚¹
- ã‚·ã‚¹ãƒ†ãƒ ã¯æ•´ç†ã®ãŸã‚ã«ä»¥ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã™ï¼š
  - `dl/`ï¼šãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«
  - `text/`ï¼šPDFã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆ
  - `summary/`ï¼šç”Ÿæˆã•ã‚ŒãŸè¦ç´„
  - `processed/`ï¼šå‡¦ç†æ¸ˆã¿è«–æ–‡ã®è¨˜éŒ²
  - `logs/`ï¼šå®Ÿè¡Œãƒ­ã‚°

- TwitteræŠ•ç¨¿ã¯`--skip-twitter`ãƒ•ãƒ©ã‚°ã§ç„¡åŠ¹ã«ã§ãã¾ã™ï¼ˆè«–æ–‡ã®åé›†ã¨è¦ç´„ã®ã¿ã‚’è¡Œã„ãŸã„å ´åˆï¼‰ã€‚

- Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯æŒ‡å®šã•ã‚ŒãŸå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç”Ÿæˆã•ã‚Œã€ä»»æ„ã®Webã‚µãƒ¼ãƒãƒ¼ã§æä¾›ã§ãã¾ã™ã€‚

- ã‚·ã‚¹ãƒ†ãƒ ã¯TwitteræŠ•ç¨¿ã¨Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ãƒã‚¹ã‚³ãƒƒãƒˆã¨ã—ã¦ã€ŒC(ãƒ»Ï‰ãƒ» )ã¤ã€ã¨ã„ã†ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯LICENSEãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚